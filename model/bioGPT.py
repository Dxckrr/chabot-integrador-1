import torch
from transformers import BioGptTokenizer, BioGptForCausalLM, set_seed
from googletrans import Translator

# Trasnlatorrr
translator = Translator()

# Model && Tokenizer
# Tokenizer => LLM word division 
tokenizer = BioGptTokenizer.from_pretrained("microsoft/biogpt")
model = BioGptForCausalLM.from_pretrained("microsoft/biogpt")

def get_chatbot_response(sentence_original):
    # input es -> () -> en
    sentence_en = translator.translate(sentence_original, src='es', dest='en').text
    inputs = tokenizer(sentence_en, return_tensors="pt")

    # Seed generated by pre stat. As a randomic number that helps us to generate outputs 
    set_seed(42)

    # OUTPUTTT
    with torch.no_grad():
        beam_output = model.generate(**inputs,
                                     min_length=50,
                                     max_length=100,
                                    #  num_beams=3,
                                     num_beams=5,    # Caminos y/o posibilidades de outputs
                                     top_k=50,       # Limita las palabras candidatas
                                     early_stopping=True
                                     )

    # Decode first output from the array
    #Output always cames given by an array of answers but we take [0] 'cause is the one that has more numeric value in our relational process
    generated_text_english = tokenizer.decode(beam_output[0], skip_special_tokens=True)

    # output en -> () -> es
    translated_text = translator.translate(generated_text_english, src='en', dest='es').text

    return translated_text
